LLM - Phase 1 - Steps


1. Install Python

Open your terminal and run:
sudo apt update
sudo apt install python3 python3-pip


2. Install Streamlit and FastAPI

Run:
pip3 install streamlit fastapi uvicorn requests


3. Install Ollama

Follow the official instructions:
https://ollama.com/download

For Linux, you can use:
curl -fsSL https://ollama.com/install.sh | sh


4. Start Ollama and Pull a Model

Start Ollama:
ollama serve

Pull a model (e.g., llama2):
ollama pull llama2


5. Create FastAPI Backend

Create a file backend.py:
import requests
from fastapi import FastAPI, Request
from fastapi.responses import JSONResponse

app = FastAPI()

@app.post("/chat")
async def chat(request: Request):
    data = await request.json()
    prompt = data.get("prompt", "")
    external_url = "http://localhost:11434/api/chat"
    
    payload = {
        "model": "llama2",
        "messages": [
            {"role": "user", "content": prompt}
        ],
        "stream": False  # This avoids multiline JSON issue
    }

    try:
        response = requests.post(external_url, json=payload)
        print("Raw response:", response.text)

        if response.status_code == 200:
            return response.json()
        else:
            return JSONResponse(
                status_code=response.status_code,
                content={"error": "Upstream error", "details": response.text}
            )

    except Exception as e:
        return JSONResponse(
            status_code=500,
            content={"error": f"Request failed: {e}"}
        )

Start FastAPI server:
uvicorn backend:app --reload


6. Create Streamlit Frontend

Create a file app.py:
import streamlit as st
import requests

st.title("My AI Chat Assistant")

# Initialize session state
if "messages" not in st.session_state:
    st.session_state.messages = []

# Chat input (Enter to send)
user_input = st.chat_input("Type your message here...")

# If user sends message
if user_input:
    # Add user message
    st.session_state.messages.append(("You", user_input))

    # Call backend
    try:
        response = requests.post("http://localhost:8000/chat", json={"prompt": user_input})
        if response.status_code == 200 and response.headers.get("Content-Type", "").startswith("application/json"):
            response_json = response.json()
            answer = response_json.get("message", {}).get("content", "")
        else:
            st.error(f"Error: {response.status_code} - {response.text}")
            answer = ""
    except Exception as e:
        st.error(f"Request failed: {e}")
        answer = ""

    # Add LLM response
    st.session_state.messages.append(("LLM", answer))

# Display messages
for sender, msg in st.session_state.messages:
    with st.chat_message(sender.lower() if sender in ["You", "LLM"] else "user"):
        st.markdown(msg)

Run Streamlit:
streamlit run app.py